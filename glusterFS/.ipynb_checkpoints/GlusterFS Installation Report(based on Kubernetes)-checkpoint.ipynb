{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlusterFS安装报告（基于Kubernetes）\n",
    "\n",
    "## 什么是GlusterFS\n",
    "\n",
    ">**GlusterFS (Gluster File System)** 是一个开源的分布式文件系统，主要由 Z RESEARCH 公司负责开发。GlusterFS 是 Scale-Out 存储解决方案 Gluster 的核心，具有强大的横向扩展能力，通过扩展能够支持数PB存储容量和处理数千客户端。GlusterFS 借助 TCP/IP 或 InfiniBand RDMA 网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据。GlusterFS 基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能。\n",
    "\n",
    "GlusterFS由存储服务器（Brick Server），客户端以及NFS/Samba存储网关组成。GlusterFS最大的设计特点就是其没有元数据（metadata）存储组件。这样的设计对整个系统性能，可靠性和稳定性的提升都具有非常重大的意义。\n",
    "\n",
    "接下来简要说明一下Glusterfs中的一些要用到的专业术语：\n",
    "\n",
    "* **Brick**：最基本的存储单元，供客户端挂载使用。\n",
    "\n",
    "* **Fuse**：类Unix操作系统中的一个可动态加载的模块，用以允许用户不用修改内核即可创建自己的文件系统。\n",
    "\n",
    "* **PV(phyical volume)**：即物理卷，是**lvm（逻辑卷管理系统）**中的概念，类似于物理磁盘上的分区，其基本组成单位是物理快（PE），其默认大小为4MB。\n",
    "\n",
    "* **VG(volumes group)**：即**卷组**，由物理卷构成，是逻辑卷的组成部分。\n",
    "\n",
    "* **LV(Logistic Volume)逻辑卷**：是lvm中最上层的文件卷，由若干个卷组中的物理卷构成，其大小可以动态指定，也能够扩大或缩小。\n",
    "\n",
    "Glusterfs一共支持七种挂载卷的形式，包括分布式卷，条带卷，副本卷等多种类型，具体的内容可参见[基于 GlusterFS 实现 Docker 集群的分布式存储](https://www.ibm.com/developerworks/cn/opensource/os-cn-glusterfs-docker-volume/index.html?lnk=hm)这篇文章。\n",
    "\n",
    "## 基于Kubernetes的GlusterFS的安装\n",
    "\n",
    "### 安装环境\n",
    "\n",
    "```\n",
    "NAME   STATUS    ROLES    VERSION   EXTERNAL-IP    OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\n",
    "lab1   Ready     node     v1.8.2    192.168.1.1    Ubuntu 16.04.4 LTS   4.4.0-62-generic   docker://17.11.0-ce\n",
    "lab2   Ready     node     v1.8.2    192.168.1.2    Ubuntu 16.04.2 LTS   4.4.0-62-generic   docker://17.11.0-ce\n",
    "lab3   Ready     node     v1.8.2    192.168.1.3    Ubuntu 16.04.2 LTS   4.4.0-62-generic   docker://17.11.0-ce\n",
    "lab4   Ready     master   v1.8.2    192.168.1.4    Ubuntu 16.04.4 LTS   4.4.0-62-generic   docker://17.11.0-ce\n",
    "```\n",
    "\n",
    "可以看到，安装环境中一共有四个节点，其中lab4为主节点，其余三个节点为从节点。每个节点上都运行Ubuntu16.04的操作系统。**（注意：GlusterFS安装要求必须至少有三个存储节点）**\n",
    "\n",
    "### GlusterFS集群搭建\n",
    "\n",
    "如果计算机中没有预先编写好的yaml文件，可以从git上下载官方的配置仓库，在cmd中运行`git clone https://github.com/gluster/gluster-kubernetes`即可下载到本地，其具体的yaml文件在配置仓库的`./deploy/kube-templates`中。虽然GlusterFS提供安装指南，但还是推荐使用预先编写好的yaml文件配置GlusterFS，因为官方仓库里下载的配置文件还需要一定的更改才能够正确运行在本地集群上。下面的安装全都假定操作者已经拥有预先编写好的yaml文件或者已经对git上下载的配置文件进行了正确修改。\n",
    "\n",
    "首先查看所有四个节点的hosts文件，需要在各个节点的hosts文件中添加其余节点的主机名，ip地址映射列表。例如在笔者所在的集群环境下hosts文件的内容：\n",
    "\n",
    "```\n",
    "root@lab4:/# cat /etc/hosts\n",
    "127.0.0.1\tlocalhost\n",
    "192.168.1.4     lab4\n",
    "192.168.1.1     lab1\n",
    "192.168.1.2     lab2\n",
    "192.168.1.3     lab3\n",
    "```\n",
    "\n",
    "配置完成hosts文件后需要在各个运行GlusterFS的节点上下载Glusterfs-client客户端进程。**（注意：不是glusterfs-server进程）**下载Glusterfs-client之前首先要更新软件源，在这里笔者更新的软件源为3.12版本，然后更新软件源。代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab3:/# echo deb http://ppa.launchpad.net/gluster/glusterfs-3.12/ubuntu xenial main >> /etc/apt/sources.list\n",
    "root@lab3:/# echo deb-src http://ppa.launchpad.net/gluster/glusterfs-3.12/ubuntu xenial main >> /etc/apt/sources.list\n",
    "root@lab3:/# apt-get update\n",
    "```\n",
    "\n",
    "**（注意：软件源更新时可能会出现403错误，这并不影响下载glusterfs客户端程序，不用理会）**更新完成之后就可以进行客户端程序的下载了，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab3:/# apt-get install glusterfs-client=3.7.6-1ubuntu1\n",
    "```\n",
    "\n",
    "通常下载完成之后glusterfs-client程序就自动开始运行，不需要手动启动。需要特别说明的是只有需要运行作为GlusterFS存储节点的物理机需要下载并安装GlusterFS-client，例如在本例中lab4作为kubernetes的主节点并不需要承担存储任务，因此就没有安装glusterfs-client程序（当然，装上也并不会有错）。\n",
    "\n",
    "执行到这一步，存储节点上的glusterfs设置就已经完成了。接下来的操作都主要在kubernetes主节点（以下均称为主节点，该节点不承担glusterfs存储任务），在本例中即lab4节点上进行。首先查看主节点上glusterfs集群的全部yaml文件：\n",
    "\n",
    "* **glusterfs-daemonset.yaml：**该文件用于创建glusterfs管理程序容器所在pod，可以理解成glusterfs的服务端，每一个glusterfs存储节点上必须有一个该类型的pod在运行。需要特别说明的是，该daemonset中采用的镜像是基于centos的，不过这并不妨碍其运行在Ubuntu系统上。\n",
    "\n",
    "* **heketi-deployment.yaml：**该文件用于创建heketi容器，该容器内运行kehekti服务进程，该进程为用户提供RESTful API，允许用户操作glusterfs集群，这便于我们自动化地管理GlusterFS集群。\n",
    "\n",
    "* **heketi-service-account.yaml：**该文件用于创建heketi服务容器所需的ServiceAccount，以及该ServiceAccount所需的用户角色，权限。由于heketi提供RESTful API允许用户管理GlusterFS集群，这就必然要求heketi操作kubernetes的某些api。基于kubernetes本身的授权机制，需要为heketi创建服务账户，并且绑定一定的权限。\n",
    "\n",
    "* **topology.json：**该文件用于描述GlusterFS集群的存储节点的拓扑结构以及所用的物理卷位置，该文件需要根据集群的实际情况编写。<\n",
    "\n",
    "* **glusterfs-storageClass.yaml：**该文件用于创建storageclass对象，该对象用于创建kubernetes中的pv对象。\n",
    "\n",
    "* **glusterfs-pvc.yaml：**该文件用于创建pvc对象，在创建过程中会自动创建pv对象。\n",
    "\n",
    "在开始GlusterFS安装前还需要做一项准备工作，由上面的文件清单可以看出glusterfs管理进程的pod是由daemonset对象建立的，之所以由该对象建立而不是ReplicationController对象建立是因为daemonset对象可以确保在每个目标物理机上只分配一个pod，该机制是基于node标签的，在本例中需要为每个运行glusterfs管理进程pod的节点都打上`storagenode=glusterfs`的标签，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/# kubectl label node lab1 storagenode=glusterfs\n",
    "root@lab4:/# kubectl label node lab2 storagenode=glusterfs\n",
    "root@lab4:/# kubectl label node lab3 storagenode=glusterfs\n",
    "```\n",
    "\n",
    "标签设置完成后就可以正式开始GlusterFS安装了，首先在各个存储节点上运行GlusterFS管理进程pod，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl create -f glusterfs-daemonset.yaml\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# daemonset \"glusterfs\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get po -o wide\n",
    "NAME                            READY     STATUS    RESTARTS   AGE       IP               NODE\n",
    "glusterfs-7fpsq                 1/1       Running   0          1d        192.168.1.2      lab2\n",
    "glusterfs-fg6z9                 1/1       Running   0          1d        192.168.1.3      lab3\n",
    "glusterfs-w68w4                 1/1       Running   0          1d        192.168.1.1      lab1\n",
    "```\n",
    "\n",
    "可以看到，三个GlusterFS管理程序容器的pod都已经正常启动，若READY容器的数目没有达到1则可以多等待一会儿。\n",
    "\n",
    "接下来开始启动heketi服务。首先设置heketi服务的ServiceAccount以及设置相关权限，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl create -f heketi-service-account.yaml\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# ServiceAccount \"heketi-service-account\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# ClusterRole \"heketi\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# ClusterRoleBinding \"heketi\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get serviceaccount\n",
    "NAME                     SECRETS   AGE\n",
    "default                  1         93d\n",
    "heketi-service-account   1         1d\n",
    "```\n",
    "\n",
    "接下来启动heketi服务，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl create -f heketi-deployment.yaml\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# Deployment \"heketi\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# Service \"heketi\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get po -o wide\n",
    "NAME                            READY     STATUS    RESTARTS   AGE       IP               NODE\n",
    "heketi-6d4bf5756b-n9bsr         1/1       Running   0          1d        10.244.202.40    lab4\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get svc\n",
    "NAME               TYPE          CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\n",
    "heketi             ClusterIP     10.107.31.73     none          8080/TCP            1d\n",
    "```\n",
    "\n",
    "可以看到已经正常启动了heketi服务的pod以及service对象。接下来需要建立GlusterFS集群，在建立集群前首先要在各存储节点上创建物理卷**（Phyical Volume）**，在本例中尝试在物理设备`/dev/sdb`上创建物理卷，并加载内核模块。代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab1:/# pvcreate -ff --metadatasize=128M --dataalignment=256K /dev/sdb1\n",
    "root@lab1:/# modprobe dm_snapshot && modprobe dm_mirror && modprobe dm_thin_pool\n",
    "root@lab2:/# pvcreate -ff --metadatasize=128M --dataalignment=256K /dev/sdb\n",
    "root@lab2:/# modprobe dm_snapshot && modprobe dm_mirror && modprobe dm_thin_pool\n",
    "root@lab3:/# pvcreate -ff --metadatasize=128M --dataalignment=256K /dev/sdb\n",
    "root@lab3:/# modprobe dm_snapshot && modprobe dm_mirror && modprobe dm_thin_pool\n",
    "```\n",
    "\n",
    "需要特别注意的是，本例中在lab1上是在物理设备`/dev/sdb1`上创建物理卷，这只是笔者自己的设定，实际操作中完全可以使用所有节点的`/dev/sdb`设备创建物理卷。若想要和笔者一样在`/dev/sdb1`创建pv，则需要首先在`/dev/sdb`上创建`sdb1`分区，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab1:/# fdisk /dev/sdb\n",
    "```\n",
    "\n",
    "接下来需要建立GlusterFS集群，首先需要进入heketi容器内部，然后创建`topology.json`文件，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl exec -it heketi-6d4bf5756b-n9bsr /bin/bash\n",
    "[root@heketi-6d4bf5756b-n9bsr /]# cat topology.json #这里默认为已经写入，实际需要用户自己设定集群拓扑结构。\n",
    "{\n",
    "  \"clusters\": [\n",
    "    {\n",
    "      \"nodes\": [\n",
    "        {\n",
    "          \"node\": {\n",
    "            \"hostnames\": {\n",
    "              \"manage\": [\n",
    "                \"lab1\"\n",
    "              ],\n",
    "              \"storage\": [\n",
    "                \"192.168.1.1\"\n",
    "              ]\n",
    "            },\n",
    "            \"zone\": 1\n",
    "          },\n",
    "          \"devices\": [\n",
    "            \"/dev/sdb1\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"node\": {\n",
    "            \"hostnames\": {\n",
    "              \"manage\": [\n",
    "                \"lab2\"\n",
    "              ],\n",
    "              \"storage\": [\n",
    "                \"192.168.1.2\"\n",
    "              ]\n",
    "            },\n",
    "            \"zone\": 1\n",
    "          },\n",
    "          \"devices\": [\n",
    "            \"/dev/sdb\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"node\": {\n",
    "            \"hostnames\": {\n",
    "              \"manage\": [\n",
    "                \"lab3\"\n",
    "              ],\n",
    "              \"storage\": [\n",
    "                \"192.168.1.3\"\n",
    "              ]\n",
    "            },\n",
    "            \"zone\": 1\n",
    "          },\n",
    "          \"devices\": [\n",
    "            \"/dev/sdb\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "[root@heketi-6d4bf5756b-n9bsr /]# export HEKETI_CLI_SERVER=http://localhost:8080\n",
    "[root@heketi-6d4bf5756b-n9bsr /]# heketi-cli topology load --json=topology.json\n",
    "Creating cluster ... ID: 063b9c0b0f9833009e3a5aae15f39b97\n",
    "        Creating node lab1 ... ID: 380d155956b3dae1d9da9fc3fa62342d\n",
    "                Adding device /dev/sdb1 ... OK\n",
    "        Creating node lab2 ... ID: 70f90e18e4243aef616847c685e6bdd7\n",
    "                Adding device /dev/sdb ... OK\n",
    "        Creating node lab3 ... ID: cd136687a2f1bfa204a43d8595094627\n",
    "                Adding device /dev/sdb ... OK\n",
    "[root@heketi-6d4bf5756b-n9bsr /]# heketi-cli topology info\n",
    "\n",
    "Cluster Id: 063b9c0b0f9833009e3a5aae15f39b97\n",
    "\n",
    "    File:  true\n",
    "    Block: true\n",
    "\n",
    "    Volumes:\n",
    "\n",
    "\tName: vol_b702551f50a70c4bdca212dc6c645a89\n",
    "\tSize: 22\n",
    "\tId: b702551f50a70c4bdca212dc6c645a89\n",
    "\tCluster Id: 063b9c0b0f9833009e3a5aae15f39b97\n",
    "\tMount: 192.168.1.3:vol_b702551f50a70c4bdca212dc6c645a89\n",
    "\tMount Options: backup-volfile-servers=192.168.1.1,192.168.1.2\n",
    "\tDurability Type: replicate\n",
    "\tReplica: 3\n",
    "\tSnapshot: Disabled\n",
    "\n",
    "\t\tBricks:\n",
    "\t\t\tId: 380d155956b3dae1d9da9fc3fa62342d\n",
    "\t\t\tPath: /var/lib/heketi/mounts/vg_59fcbc365425748d6eff5d86d72537d9/brick_380d155956b3dae1d9da9fc3fa62342d/brick\n",
    "\t\t\tSize (GiB): 22\n",
    "\t\t\tNode: 1ae173fc19eabc6727ce805d024b6436\n",
    "\t\t\tDevice: 59fcbc365425748d6eff5d86d72537d9\n",
    "\n",
    "\t\t\tId: 70f90e18e4243aef616847c685e6bdd7\n",
    "\t\t\tPath: /var/lib/heketi/mounts/vg_ab3e126813240403a145d5fe23cbd2a3/brick_70f90e18e4243aef616847c685e6bdd7/brick\n",
    "\t\t\tSize (GiB): 22\n",
    "\t\t\tNode: d7f0a6e0c8868f6aa100561950cc3aab\n",
    "\t\t\tDevice: ab3e126813240403a145d5fe23cbd2a3\n",
    "\n",
    "\t\t\tId: cd136687a2f1bfa204a43d8595094627\n",
    "\t\t\tPath: /var/lib/heketi/mounts/vg_8efe43e4391a2f3688d035fd3e28e838/brick_cd136687a2f1bfa204a43d8595094627/brick\n",
    "\t\t\tSize (GiB): 22\n",
    "\t\t\tNode: d19ea140ef8394321a7a6c6df42aaef0\n",
    "\t\t\tDevice: 8efe43e4391a2f3688d035fd3e28e838\n",
    "\n",
    "\n",
    "    Nodes:\n",
    "\n",
    "\tNode Id: 1ae173fc19eabc6727ce805d024b6436\n",
    "\tState: online\n",
    "\tCluster Id: 063b9c0b0f9833009e3a5aae15f39b97\n",
    "\tZone: 1\n",
    "\tManagement Hostnames: lab3\n",
    "\tStorage Hostnames: 192.168.1.3\n",
    "\tDevices:\n",
    "\t\tId:59fcbc365425748d6eff5d86d72537d9   Name:/dev/sdb            State:online    Size (GiB):279     Used (GiB):0      Free (GiB):279     \n",
    "\t\t\tBricks:\n",
    "\n",
    "\tNode Id: d19ea140ef8394321a7a6c6df42aaef0\n",
    "\tState: online\n",
    "\tCluster Id: 063b9c0b0f9833009e3a5aae15f39b97\n",
    "\tZone: 1\n",
    "\tManagement Hostnames: lab1\n",
    "\tStorage Hostnames: 192.168.1.1\n",
    "\tDevices:\n",
    "\t\tId:8efe43e4391a2f3688d035fd3e28e838   Name:/dev/sdb1           State:online    Size (GiB):279     Used (GiB):0      Free (GiB):279     \n",
    "\t\t\tBricks:\n",
    "\n",
    "\tNode Id: d7f0a6e0c8868f6aa100561950cc3aab\n",
    "\tState: online\n",
    "\tCluster Id: 063b9c0b0f9833009e3a5aae15f39b97\n",
    "\tZone: 1\n",
    "\tManagement Hostnames: lab2\n",
    "\tStorage Hostnames: 192.168.1.2\n",
    "\tDevices:\n",
    "\t\tId:ab3e126813240403a145d5fe23cbd2a3   Name:/dev/sdb            State:online    Size (GiB):279     Used (GiB):0      Free (GiB):279     \n",
    "\t\t\tBricks:\n",
    "```\n",
    "\n",
    "可以看到，GlusterFS集群已经创建成功，并且能够通过`heketi-cli topology info`命令查看到相应的集群信息。\n",
    "\n",
    "接下来需要创建storageClass对象。特别要注意的是在storageClass对象的yaml文件中需要动态配置heketi服务的地址，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# cat glusterfs-storageClass.yaml \n",
    "kind: StorageClass\n",
    "apiVersion: storage.k8s.io/v1beta1\n",
    "metadata:\n",
    "  name: gluster-yh\n",
    "  labels:\n",
    "    glusterfs: s3-sc\n",
    "    gluster-s3: sc\n",
    "provisioner: kubernetes.io/glusterfs\n",
    "parameters:\n",
    "  resturl: \"http://10.107.31.73:8080\" #需要根据实际动态配置heketi服务的service地址。\n",
    "  restuser: \"admin\"\n",
    "  restauthenabled: \"false\"\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl create -f glusterfs-storageClass.yaml\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# storageclass \"gluster-yh\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get storageclass\n",
    "NAME         PROVISIONER\n",
    "fast         k8s.io/minikube-hostpath\n",
    "gluster-yh   kubernetes.io/glusterfs\n",
    "```\n",
    "\n",
    "可以看到，名为gluster-yh的storageclass对象已经建立。接下来创建pvc对象，该对象创建成功后会自动创建pv对象，代码清单如下：\n",
    "\n",
    "```\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl create -f glusterfs-pvc.yaml\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# PersistentVolumeClaim \"gluster-pvc-yh\" created\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get pvc\n",
    "NAME                STATUS    VOLUME             CAPACITY   ACCESS MODES   STORAGECLASS     AGE\n",
    "gluster-pvc-yh      Bound     pvc-4991584c-...   22Gi       RWX            gluster-yh       23h\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl get pv\n",
    "NAME         CAPACITY   ACCESS MODES  RECLAIM POLICY   STATUS   CLAIM                   STORAGECLASS  REASON  AGE\n",
    "pvc-499...   22Gi       RWX           Delete           Bound    default/gluster-pvc-yh  gluster-yh            23h\n",
    "root@lab4:/home/yanghang/source/database/glusterFS# kubectl describe pv pvc-4991584c-34cc-11e8-af63-00259081f706\n",
    "Name:            pvc-4991584c-34cc-11e8-af63-00259081f706\n",
    "Labels:          none\n",
    "Annotations:     Description=Gluster-Internal: Dynamically provisioned PV\n",
    "                 gluster.org/type=file\n",
    "                 kubernetes.io/createdby=heketi-dynamic-provisioner\n",
    "                 pv.beta.kubernetes.io/gid=2000\n",
    "                 pv.kubernetes.io/bound-by-controller=yes\n",
    "                 pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs\n",
    "                 volume.beta.kubernetes.io/mount-options=auto_unmount\n",
    "StorageClass:    gluster-yh\n",
    "Status:          Bound\n",
    "Claim:           default/gluster-pvc-yh\n",
    "Reclaim Policy:  Delete\n",
    "Access Modes:    RWX\n",
    "Capacity:        22Gi\n",
    "Message:         \n",
    "Source:\n",
    "    Type:           Glusterfs (a Glusterfs mount on the host that shares a pod's lifetime)\n",
    "    EndpointsName:  glusterfs-dynamic-gluster-pvc-yh\n",
    "    Path:           vol_b702551f50a70c4bdca212dc6c645a89\n",
    "    ReadOnly:       false\n",
    "Events:             none\n",
    "```\n",
    "\n",
    "从以上打印信息可以看出，GlusterFS存储已经完全建立好了。用户使用时只需要引用生成的pvc对象即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
