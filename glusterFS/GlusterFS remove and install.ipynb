{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GlusterFS格式化重装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GlusterFS有时会出现问题需要重新安装，本文档就是为了指导重装GlusterFS而写。\n",
    "\n",
    "## 删除相应磁盘分区上的物理卷\n",
    "\n",
    "由于GlusterFS系统是基于Linux中的逻辑卷管理（LVM）设计的，因此在重装GlusterFS时必须“格式化”GlusterFS各个节点上的LVM系统。本次重装以西工大集群为例，集群的GlusterFS共有三个节点，所有的操作均需要在三个节点上依次完成。\n",
    "\n",
    "### 查看节点上的物理卷情况\n",
    "\n",
    "以lab2为例，笔者最初在lab2上将物理卷创建在`/dev/sdb`分区上，因此查询时输入命令`pvdisplay /dev/sdb`来查看`/dev/sdb`下的物理卷情况。输入命令后结果如下所示：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# pvdisplay /dev/sdb\n",
    "  /run/lvm/lvmetad.socket: connect failed: Connection refused\n",
    "  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.\n",
    "  --- Physical volume ---\n",
    "  PV Name               /dev/sdb\n",
    "  VG Name               vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  PV Size               279.40 GiB / not usable 130.29 MiB\n",
    "  Allocatable           yes \n",
    "  PE Size               4.00 MiB\n",
    "  Total PE              71493\n",
    "  Free PE               51637\n",
    "  Allocated PE          19856\n",
    "  PV UUID               tjYoeE-Xmyu-rTVx-KmnF-kPgH-holu-By2UpL\n",
    "```\n",
    "\n",
    "可以看到该物理卷所属的卷组（Volumn Group）名称为`vg_bd4d7f613494552be82a4c2699f35b5e`，要删除该物理卷就需要首先删除该卷组。\n",
    "\n",
    "### 查看节点上卷组的情况\n",
    "\n",
    "还是以上面的例子为例，我们已经知道了与物理卷有关的卷组名称为`vg_bd4d7f613494552be82a4c2699f35b5e`，现在键入命令`vgdisplay vg_bd4d7f613494552be82a4c2699f35b5e`来查看该卷组的相关信息，命令执行结果如下：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# vgdisplay -v vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  /run/lvm/lvmetad.socket: connect failed: Connection refused\n",
    "  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.\n",
    "    Using volume group(s) on command line.\n",
    "  --- Volume group ---\n",
    "  VG Name               vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  System ID             \n",
    "  Format                lvm2\n",
    "  Metadata Areas        1\n",
    "  Metadata Sequence No  183\n",
    "  VG Access             read/write\n",
    "  VG Status             resizable\n",
    "  MAX LV                0\n",
    "  Cur LV                6\n",
    "  Open LV               0\n",
    "  Max PV                0\n",
    "  Cur PV                1\n",
    "  Act PV                1\n",
    "  VG Size               279.27 GiB\n",
    "  PE Size               4.00 MiB\n",
    "  Total PE              71493\n",
    "  Alloc PE / Size       19856 / 77.56 GiB\n",
    "  Free  PE / Size       51637 / 201.71 GiB\n",
    "  VG UUID               P0bpYz-ceHZ-Gx3b-9ONv-hARN-ZXYf-oKJBp2\n",
    "   \n",
    "  --- Logical volume ---\n",
    "  LV Name                tp_9535d3570e834c1de8a41b237f6385c0\n",
    "  VG Name                vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  LV UUID                HOf2C5-fwxB-kLD5-dQDj-ujJR-BDjb-iBTZY3\n",
    "  LV Write Access        read/write\n",
    "  LV Creation host, time lab2, 2018-05-27 13:10:00 +0800\n",
    "  LV Pool metadata       tp_9535d3570e834c1de8a41b237f6385c0_tmeta\n",
    "  LV Pool data           tp_9535d3570e834c1de8a41b237f6385c0_tdata\n",
    "  LV Status              available\n",
    "  # open                 2\n",
    "  LV Size                33.00 GiB\n",
    "  Allocated pool data    3.16%\n",
    "  Allocated metadata     0.11%\n",
    "  Current LE             8448\n",
    "  Segments               1\n",
    "  Allocation             inherit\n",
    "  Read ahead sectors     auto\n",
    "  - currently set to     256\n",
    "  Block device           252:7\n",
    "   \n",
    "  --- Logical volume ---\n",
    "  LV Path                /dev/vg_bd4d7f613494552be82a4c2699f35b5e/brick_9535d3570e834c1de8a41b237f6385c0\n",
    "  LV Name                brick_9535d3570e834c1de8a41b237f6385c0\n",
    "  VG Name                vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  LV UUID                19tdwe-ZP6D-RAVa-hBoJ-wD0W-ccwd-3EKUci\n",
    "  LV Write Access        read/write\n",
    "  LV Creation host, time lab2, 2018-05-27 13:10:01 +0800\n",
    "  LV Pool name           tp_9535d3570e834c1de8a41b237f6385c0\n",
    "  LV Status              available\n",
    "  # open                 0\n",
    "  LV Size                33.00 GiB\n",
    "  Mapped size            3.16%\n",
    "  Current LE             8448\n",
    "  Segments               1\n",
    "  Allocation             inherit\n",
    "  Read ahead sectors     auto\n",
    "  - currently set to     256\n",
    "  Block device           252:9\n",
    "   \n",
    "  --- Logical volume ---\n",
    "  LV Name                tp_3d28e0f7b265ff0fae2cc736bda893cd\n",
    "  VG Name                vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  LV UUID                OhBN7O-o6JA-UIXD-bVtL-nX5s-O8SY-Eyh59c\n",
    "  LV Write Access        read/write\n",
    "  LV Creation host, time lab2, 2018-08-05 16:56:35 +0800\n",
    "  LV Pool metadata       tp_3d28e0f7b265ff0fae2cc736bda893cd_tmeta\n",
    "  LV Pool data           tp_3d28e0f7b265ff0fae2cc736bda893cd_tdata\n",
    "  LV Status              available\n",
    "  # open                 2\n",
    "  LV Size                22.00 GiB\n",
    "  Allocated pool data    16.35%\n",
    "  Allocated metadata     0.39%\n",
    "  Current LE             5632\n",
    "  Segments               1\n",
    "  Allocation             inherit\n",
    "  Read ahead sectors     auto\n",
    "  - currently set to     256\n",
    "  Block device           252:47\n",
    "   \n",
    "  --- Logical volume ---\n",
    "  LV Path                /dev/vg_bd4d7f613494552be82a4c2699f35b5e/brick_3d28e0f7b265ff0fae2cc736bda893cd\n",
    "  LV Name                brick_3d28e0f7b265ff0fae2cc736bda893cd\n",
    "  VG Name                vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  LV UUID                ZoJZnG-HA79-IvP9-V8K3-2zrw-n9B1-03lR9R\n",
    "  LV Write Access        read/write\n",
    "  LV Creation host, time lab2, 2018-08-05 16:56:36 +0800\n",
    "  LV Pool name           tp_3d28e0f7b265ff0fae2cc736bda893cd\n",
    "  LV Status              available\n",
    "  # open                 0\n",
    "  LV Size                22.00 GiB\n",
    "  Mapped size            16.35%\n",
    "  Current LE             5632\n",
    "  Segments               1\n",
    "  Allocation             inherit\n",
    "  Read ahead sectors     auto\n",
    "  - currently set to     256\n",
    "  Block device           252:49\n",
    "   \n",
    "  --- Logical volume ---\n",
    "  LV Name                tp_3f022c56a99e16108add6cde8731d80a\n",
    "  VG Name                vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  LV UUID                CQr84O-XKJO-HrUC-nIf1-PtjJ-iF4y-UN1ToH\n",
    "  LV Write Access        read/write\n",
    "  LV Creation host, time lab2, 2018-08-05 16:57:00 +0800\n",
    "  LV Pool metadata       tp_3f022c56a99e16108add6cde8731d80a_tmeta\n",
    "  LV Pool data           tp_3f022c56a99e16108add6cde8731d80a_tdata\n",
    "  LV Status              available\n",
    "  # open                 2\n",
    "  LV Size                22.00 GiB\n",
    "  Allocated pool data    14.77%\n",
    "  Allocated metadata     0.37%\n",
    "  Current LE             5632\n",
    "  Segments               1\n",
    "  Allocation             inherit\n",
    "  Read ahead sectors     auto\n",
    "  - currently set to     256\n",
    "  Block device           252:52\n",
    "   \n",
    "  --- Logical volume ---\n",
    "  LV Path                /dev/vg_bd4d7f613494552be82a4c2699f35b5e/brick_3f022c56a99e16108add6cde8731d80a\n",
    "  LV Name                brick_3f022c56a99e16108add6cde8731d80a\n",
    "  VG Name                vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  LV UUID                65PLp7-WaqE-BWoS-0CVf-Lu3F-6P5p-GwKwQj\n",
    "  LV Write Access        read/write\n",
    "  LV Creation host, time lab2, 2018-08-05 16:57:00 +0800\n",
    "  LV Pool name           tp_3f022c56a99e16108add6cde8731d80a\n",
    "  LV Status              available\n",
    "  # open                 0\n",
    "  LV Size                22.00 GiB\n",
    "  Mapped size            14.77%\n",
    "  Current LE             5632\n",
    "  Segments               1\n",
    "  Allocation             inherit\n",
    "  Read ahead sectors     auto\n",
    "  - currently set to     256\n",
    "  Block device           252:54\n",
    "   \n",
    "  --- Physical volumes ---\n",
    "  PV Name               /dev/sdb     \n",
    "  PV UUID               tjYoeE-Xmyu-rTVx-KmnF-kPgH-holu-By2UpL\n",
    "  PV Status             allocatable\n",
    "  Total PE / Free PE    71493 / 51637\n",
    "\n",
    "```\n",
    "\n",
    "可以看到该卷组一共包含六个逻辑卷和一个物理卷（即上一小节查到的），接下来我们需要删除该卷组，正常情况下删除卷组前应该卸载该卷组上的所有逻辑卷，但是与该卷组相关的逻辑卷太多，一一删除太麻烦，因此我们直接删除卷组即可，删除卷组的同时就会自动卸载与之相关的逻辑卷。\n",
    "\n",
    "### 删除卷组\n",
    "\n",
    "输入命令`vgremove vg_bd4d7f613494552be82a4c2699f35b5e`来删除该卷组，由于我们事先并未删除与之相关的逻辑卷，因此在删除的过程中会被系统询问是否删除相关的逻辑卷，全部键入`y`确定即可。命令执行结果如下：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# vgremove vg_bd4d7f613494552be82a4c2699f35b5e\n",
    "  /run/lvm/lvmetad.socket: connect failed: Connection refused\n",
    "  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.\n",
    "Do you really want to remove volume group \"vg_bd4d7f613494552be82a4c2699f35b5e\" containing 6 logical volumes? [y/n]: y\n",
    "Removing pool \"tp_9535d3570e834c1de8a41b237f6385c0\" will remove 1 dependent volume(s). Proceed? [y/n]: y\n",
    "Do you really want to remove and DISCARD active logical volume brick_9535d3570e834c1de8a41b237f6385c0? [y/n]: y\n",
    "  Logical volume \"brick_9535d3570e834c1de8a41b237f6385c0\" successfully removed\n",
    "Do you really want to remove and DISCARD active logical volume tp_9535d3570e834c1de8a41b237f6385c0? [y/n]: y\n",
    "  /usr/sbin/thin_check: execvp failed: No such file or directory\n",
    "  WARNING: Integrity check of metadata for pool vg_bd4d7f613494552be82a4c2699f35b5e/tp_9535d3570e834c1de8a41b237f6385c0 failed.\n",
    "  Logical volume \"tp_9535d3570e834c1de8a41b237f6385c0\" successfully removed\n",
    "Removing pool \"tp_3d28e0f7b265ff0fae2cc736bda893cd\" will remove 1 dependent volume(s). Proceed? [y/n]: y\n",
    "Do you really want to remove and DISCARD active logical volume brick_3d28e0f7b265ff0fae2cc736bda893cd? [y/n]: y\n",
    "  Logical volume \"brick_3d28e0f7b265ff0fae2cc736bda893cd\" successfully removed\n",
    "Do you really want to remove and DISCARD active logical volume tp_3d28e0f7b265ff0fae2cc736bda893cd? [y/n]: y\n",
    "  /usr/sbin/thin_check: execvp failed: No such file or directory\n",
    "  WARNING: Integrity check of metadata for pool vg_bd4d7f613494552be82a4c2699f35b5e/tp_3d28e0f7b265ff0fae2cc736bda893cd failed.\n",
    "  Logical volume \"tp_3d28e0f7b265ff0fae2cc736bda893cd\" successfully removed\n",
    "Removing pool \"tp_3f022c56a99e16108add6cde8731d80a\" will remove 1 dependent volume(s). Proceed? [y/n]: y\n",
    "Do you really want to remove and DISCARD active logical volume brick_3f022c56a99e16108add6cde8731d80a? [y/n]: y\n",
    "  Logical volume \"brick_3f022c56a99e16108add6cde8731d80a\" successfully removed\n",
    "Do you really want to remove and DISCARD active logical volume tp_3f022c56a99e16108add6cde8731d80a? [y/n]: y\n",
    "  /usr/sbin/thin_check: execvp failed: No such file or directory\n",
    "  WARNING: Integrity check of metadata for pool vg_bd4d7f613494552be82a4c2699f35b5e/tp_3f022c56a99e16108add6cde8731d80a failed.\n",
    "  Logical volume \"tp_3f022c56a99e16108add6cde8731d80a\" successfully removed\n",
    "  Volume group \"vg_bd4d7f613494552be82a4c2699f35b5e\" successfully removed\n",
    "```\n",
    "\n",
    "如上面代码所示，卷组已经成功删除。接下来需要删除物理卷。\n",
    "\n",
    "### 删除物理卷\n",
    "\n",
    "删除卷组后就可以直接删除物理卷了，命令行中键入命令`pvremove /dev/sdb`即可执行删除，执行结果如下面所示：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# pvremove /dev/sdb\n",
    "  /run/lvm/lvmetad.socket: connect failed: Connection refused\n",
    "  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.\n",
    "  Labels on physical volume \"/dev/sdb\" successfully wiped\n",
    "```\n",
    "\n",
    "以上的步骤需要在所有的GlusterFS节点上依次执行，执行时注意物理卷名称，卷组名称等信息，请根据节点的实际情况填写。\n",
    "\n",
    "### 格式化分区（选做，可以尝试，做了无害）\n",
    "\n",
    "为了彻底消除磁盘上旧的数据，应格式化GlusterFS的硬盘分区，执行`mkfs.ext4 /dev/vdb`命令，执行后的结果如下：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# mkfs.ext4 /dev/sdb\n",
    "mke2fs 1.42.13 (17-May-2015)\n",
    "Creating filesystem with 73242187 4k blocks and 18317312 inodes\n",
    "Filesystem UUID: 34167c08-516a-46b3-8935-5327e2627db6\n",
    "Superblock backups stored on blocks: \n",
    "\t32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n",
    "\t4096000, 7962624, 11239424, 20480000, 23887872, 71663616\n",
    "\n",
    "Allocating group tables: done                            \n",
    "Writing inode tables: done                            \n",
    "Creating journal (32768 blocks): done\n",
    "Writing superblocks and filesystem accounting information: done     \n",
    "```\n",
    "\n",
    "## 删除旧的各个从节点上旧的GlusterFS客户端（选做，若卸载不成功可以尝试这一步）\n",
    "\n",
    "为了彻底删除各个从节点上的GlusterFS的数据，应该重装GlusterFS客户端，执行命令`apt-get remove --purge glusterfs-client`及`apt-get remove glusterfs-client`，执行结果如下：\n",
    "\n",
    "```\n",
    "root@lab2:/var/lib# apt-get remove --purge glusterfs-client\n",
    "Reading package lists... Done\n",
    "Building dependency tree       \n",
    "Reading state information... Done\n",
    "The following packages were automatically installed and are no longer required:\n",
    "  glusterfs-common librdmacm1 liburcu4 python-prettytable\n",
    "Use 'sudo apt autoremove' to remove them.\n",
    "The following packages will be REMOVED:\n",
    "  glusterfs-client*\n",
    "0 upgraded, 0 newly installed, 1 to remove and 289 not upgraded.\n",
    "After this operation, 50.2 kB disk space will be freed.\n",
    "Do you want to continue? [Y/n] y\n",
    "(Reading database ... 82230 files and directories currently installed.)\n",
    "Removing glusterfs-client (3.12.9-ubuntu1~xenial1) ...\n",
    "Purging configuration files for glusterfs-client (3.12.9-ubuntu1~xenial1) ...\n",
    "Processing triggers for man-db (2.7.5-1) ...\n",
    "root@lab2:/var/lib# apt-get remove glusterfs-client\n",
    "Reading package lists... Done\n",
    "Building dependency tree       \n",
    "Reading state information... Done\n",
    "Package 'glusterfs-client' is not installed, so not removed\n",
    "The following packages were automatically installed and are no longer required:\n",
    "  glusterfs-common librdmacm1 liburcu4 python-prettytable\n",
    "Use 'sudo apt autoremove' to remove them.\n",
    "0 upgraded, 0 newly installed, 0 to remove and 289 not upgraded.|\n",
    "```\n",
    "\n",
    "## 创建新的物理卷\n",
    "\n",
    "删除了旧的物理卷相当于格式化硬盘，现在需要在各个节点上重新创建物理卷，输入命令如下：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# pvcreate -ff --metadatasize=128M --dataalignment=256K /dev/sdb\n",
    "  /run/lvm/lvmetad.socket: connect failed: Connection refused\n",
    "  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.\n",
    "  Physical volume \"/dev/sdb\" successfully created\n",
    "root@lab2:/dev# modprobe dm_snapshot && modprobe dm_mirror && modprobe dm_thin_pool\n",
    "```\n",
    "\n",
    "如上面代码所示，新的物理卷已经创建成功。键入命令`pvdisplay /dev/sdb`来查看创建情况：\n",
    "\n",
    "```\n",
    "root@lab2:/dev# pvdisplay /dev/sdb\n",
    "  /run/lvm/lvmetad.socket: connect failed: Connection refused\n",
    "  WARNING: Failed to connect to lvmetad. Falling back to internal scanning.\n",
    "  \"/dev/sdb\" is a new physical volume of \"279.40 GiB\"\n",
    "  --- NEW Physical volume ---\n",
    "  PV Name               /dev/sdb\n",
    "  VG Name               \n",
    "  PV Size               279.40 GiB\n",
    "  Allocatable           NO\n",
    "  PE Size               0   \n",
    "  Total PE              0\n",
    "  Free PE               0\n",
    "  Allocated PE          0\n",
    "  PV UUID               jVGl1F-CEEp-M3pf-goLd-ocni-Auim-wxOdmG\n",
    "```\n",
    "\n",
    "如上面代码所示，物理卷已经创建成功。\n",
    "\n",
    "## 删除heketi的数据\n",
    "\n",
    "要想完全重装就必须删除heketi的历史数据，首先打开`heketi-deployment.yaml`文件，查看heketi的数据保存在哪里，以下做简要展示：\n",
    "\n",
    "```\n",
    "root@lab4:/home/presentation/NWPU/resource/glusterfs# vim heketi-deployment.yaml\n",
    " ...\n",
    "      volumes:\n",
    "      - name: db\n",
    "        hostPath:\n",
    "          path: \"/heketi-data\"\n",
    " ...\n",
    "```\n",
    "\n",
    "随后删除该path下的目录：`rm -rf /heketi-data`\n",
    "\n",
    "接下来的步骤按照文档“GlusterFS安装报告（基于Kubernetes）”中所描述的那样执行即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
